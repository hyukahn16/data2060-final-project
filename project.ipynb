{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105f1e4-612d-4c5a-a890-66c26f9cf031",
   "metadata": {},
   "source": [
    "### <ins>Overview of Multiclass Classification: **one-vs-all** and **all-pairs**\n",
    "\n",
    "Give an overview of the algorithm and describe its advantages and disadvantages.\n",
    "\n",
    "#### <ins>One-vs-All\n",
    "\n",
    "This algorithm creates a classifier for each class (3 classifiers if there exists 3 classes). For each classifier, it is responsible for predicting whether an input belongs to its corresponding class or not.\n",
    "\n",
    "Each classifier is trained on the entire dataset with modifications corresponding to each classifier.\\\n",
    "The modification changes the dataset such that when you're training a classifier for class $0$, labels for all the other classes are modified to $-1$ and labels for the classifier's class are modified to $1$. (More details will be provided in the Representation section)\n",
    "\n",
    "#### <ins>All-Pairs\n",
    "\n",
    "This algorithm creates a classifier for each pair of classes. For each classifier, it is responsible for predicting whether a given input belongs to \n",
    "\n",
    "#### <ins>Advantages and Disadvantages of Multiclass Classification\n",
    "\n",
    "Multiclass classification algorithm is an algorithm that classifies an input that can belong to one of the multiple classes (more than two classes).\\\n",
    "For this project, we will be implementing One-vs-All and All-Pairs algorithms for the multiclass classification of the UCI Iris dataset.\n",
    "\n",
    "Compared to a multiclass classification algorithm that inherently encompasses multiclass classification (output of model predicts multiclass),\n",
    "the main advantages of One-vs-All and All-Pairs stems from the use of binary classifiers.\\\n",
    "Because of the binary classifiers to represent multiclass classification, these two algorithms have implementation simplicity and easy interpretability of the predictions.\n",
    "\n",
    "Unfortunately, the disadvantages also stem from the use of binary classifiers.\n",
    "- The binary classifiers do not have any knowledge that it is used for multiclass classification and therefore, does not have inherent understanding of the multiclass classification problem.\n",
    "- Due to training classifier for each class, each classifier is trained on a class imbalanced dataset and may result in overfitting.\n",
    "- Training multiple classifiers can be computationally expensive.\n",
    "\n",
    "#### <ins>Misc.\n",
    "\n",
    "In this final project, we will be using the UCI Iris dataset we encountered in our previous homework:\\\n",
    "[`https://archive.ics.uci.edu/dataset/53/iris`](https://archive.ics.uci.edu/dataset/53/iris)\n",
    "\n",
    "What we will be comparing to:\n",
    "[scikit-learn multiclass classification](https://scikit-learn.org/1.5/modules/multiclass.html#multilabel-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ba03-9fb9-49ce-8201-a319beb7f36f",
   "metadata": {},
   "source": [
    "### Representation: Logistic Regression\n",
    "\n",
    "describe how the feature values are converted into a single number prediction.\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Given sample's feature values $x \\in \\mathbb{R}^{d}$ and a label $y  \\in \\{-1, 1\\}$, classification of input $x$ is predicted through combination of affine function and sigmoid function.\n",
    "$$ \\langle w, x\\rangle + b = y \\text{  and  } \\sigma = \\frac{1}{1 + e^{-y}}$$\n",
    "\n",
    "#### Multiclass Logistic Regression\n",
    "one-vs-all\n",
    "\n",
    "all-pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa54a1c-c90e-488b-bdb1-265b147f26f8",
   "metadata": {},
   "source": [
    "### Loss: Logistic Loss + Regularization\n",
    "\n",
    "The loss function of a Logistic Regression classifier over $k$ classes is the **log-loss**, also called **cross-entropy loss**. Since we will only use binary classifier, e.g. Binary Logistic Regression, in this project, only **Binary Log Loss** will be introduced in this section.\n",
    "\n",
    "The Binary Log Loss on a sample of m data points, also called the Binary Cross Entropy Loss, is:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "The corresponding gradient of the Binary Log loss with respect to the model's wights is:\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "We also implement the L2 norm of wights to adpot Tikhonov regularization into our loss function. The L2 norm of wights is:\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "And the gradient of the L2 term with respect to the model's weights is:\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "\n",
    "In conclusion, the total loss function would be:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))+ \\lambda\\sum_{i=1}^{d}w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622a64e-b029-4a79-8568-f9518a31350d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**One-vs-All** and **All-Pairs** are both strategies used to solve muticlass classification problems by utilizing binary classifiers. In this case, Stochastic Gradient Descent (Mini Batch) is a suitable choice.   \n",
    "In gradient descent, the general formula for weight update is:\n",
    "$$w_j = w_j - \\alpha \\cdot \\frac{\\partial L}{\\partial w_j}$$  \n",
    "\n",
    "For each batch of size $m$, the gradient of the binary log loss with respect to the weight is:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij}$$  \n",
    "\n",
    "If incorporate regularization (mentioned in the previous section), the total gradient becomes:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij} + 2 \\lambda w_j$$\n",
    "Thus, the final weight update equation is: \n",
    "$$w_j = w_j - \\alpha \\cdot \\left( \\frac{\\partial L}{\\partial w_j} + 2 \\lambda w_j \\right)$$  \n",
    "\n",
    "Due to the nature of One-vs-All and All-pairs strategies, we apply this optimizer differently compared to direct multiclass classification techniques, such as multiclass logistic regression.  \n",
    "**One-vs-All**: for each class $j$, you train a seperate binary classifier that distinguishes class $j$ from all other classes.  \n",
    "**All-pairs**: for each unique class pair $(i, j)$, you train a binary classifier that differentiates between those two classes.  \n",
    "\n",
    "#### Pseudocode: Stochastic Gradient Descent for Logistic Regression (Lecture 6 Slide 21)  \n",
    "****Inputs**: Traning examples $S$, step size $\\alpha$, batch size $b < |S|$  \n",
    "Set converged false  \n",
    "**while** not converged:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Randomly shuffle $S$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $i = 0$ to $|S|/b - 1$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S'$ = Extracted current batch using $i$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\nabla L_{S'}(h_w)$ + regularization  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;converged = check_convergence$(S,w)$  \n",
    "return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1aa45-ab5f-4556-a4b1-baee46a7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
