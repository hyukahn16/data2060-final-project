{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105f1e4-612d-4c5a-a890-66c26f9cf031",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification: one-vs-all and all-pairs**\n",
    "\n",
    "Give an overview of the algorithm and describe its advantages and disadvantages.\n",
    "\n",
    "#### <ins>One-vs-All\n",
    "\n",
    "This algorithm creates a classifier for each class (3 classifiers if there exists 3 classes). For each classifier, it is responsible for predicting whether an input belongs to its corresponding class or not.\n",
    "\n",
    "Each classifier is trained on the entire dataset with modifications corresponding to each classifier.\\\n",
    "The modification changes the dataset such that when you're training a classifier for class $3$, labels for all the other classes are modified to $-1$ and labels for the classifier's class are modified to $1$. (More details will be provided in the Representation section)\n",
    "\n",
    "#### <ins>All-Pairs\n",
    "\n",
    "This algorithm creates a classifier for each pair of classes. For each classifier, it is responsible for predicting whether a given input belongs to one class or the other.\n",
    "\n",
    "Each classifier is trained a portion of the dataset with modifications corresponding to each classifier.\\\n",
    "First, each classifier is assigned portion of the dataset that contains the classes the classifier is predicting for. Then, the assigned data's classes are changed so that one class is assigned the label of $1$ and the other is assigned the label of $-1$.\n",
    "\n",
    "#### <ins>Advantages and Disadvantages of Multiclass Classification\n",
    "\n",
    "Multiclass classification algorithm is an algorithm that classifies an input that can belong to one of the multiple classes (more than two classes).\\\n",
    "For this project, we will be implementing One-vs-All and All-Pairs algorithms for the multiclass classification of the UCI Iris dataset.\n",
    "\n",
    "Compared to a multiclass classification algorithm that inherently encompasses multiclass classification (output of model predicts multiclass),\n",
    "the main advantages of One-vs-All and All-Pairs stems from the use of binary classifiers.\\\n",
    "Because of the binary classifiers to represent multiclass classification, these two algorithms have implementation simplicity and easy interpretability of the predictions.\n",
    "\n",
    "Unfortunately, the disadvantages also stem from the use of binary classifiers.\n",
    "- The binary classifiers do not have any knowledge that it is used for multiclass classification and therefore, does not have inherent understanding of the multiclass classification problem.\n",
    "- Due to training classifier for each class, each classifier is trained on a class imbalanced dataset and may result in overfitting.\n",
    "- Training multiple classifiers can be computationally expensive.\n",
    "\n",
    "#### <ins>Misc.\n",
    "\n",
    "In this final project, we will be using the UCI Iris dataset we encountered in our previous homework:\\\n",
    "[`https://archive.ics.uci.edu/dataset/53/iris`](https://archive.ics.uci.edu/dataset/53/iris)\n",
    "\n",
    "What we will be comparing to:\n",
    "[scikit-learn multiclass classification](https://scikit-learn.org/1.5/modules/multiclass.html#multilabel-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ba03-9fb9-49ce-8201-a319beb7f36f",
   "metadata": {},
   "source": [
    "### Representation: Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Given sample's feature values $x \\in \\mathbb{R}^{d}$ and a label $y  \\in \\{0, 1\\}$, binary classification of input $x$ is predicted through combination of affine function and sigmoid function.\n",
    "$$ y = \\langle w, x\\rangle $$ \n",
    "$$\\sigma (y) = \\frac{1}{1 + e^{-y}}$$\n",
    "Therefore, our hypothesis function defined on weights $w$ is\n",
    "$$h_{w}(x) = \\frac{1}{1 + e^{-\\langle w, x\\rangle}}$$\n",
    "\n",
    "#### Multiclass Logistic Regression\n",
    "Now, using the binary logistic regression defined above, we will define one-vs-all and all-pairs multiclass logistic regression algorithms.\n",
    "\n",
    "Pseudocode for one-vs-all (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i \\in Y:$\\\n",
    "$\\text{ let } S_i = (x_1, (-1)^{\\mathbb{1}_{[y_1 \\neq i]}}), ..., (x_m, (-1)^{\\mathbb{1}_{[y_m \\neq i]}})$\\\n",
    "$\\text{ let } h_i = L(S_i)$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ }h_i(x)$\n",
    "\n",
    "\n",
    "Pseudocode for all-pairs (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i,j \\in Y \\text{ such that } i < j$\\\n",
    "$\\text{ initialize empty } S_{i,j}$\\\n",
    "$\\text{ for } t = 1, ..., m$\\\n",
    "$\\text{ }\\text{ If } y_t = i \\text{, then add } (x_t, 1) \\text{ to } S_{i,j}$\n",
    "$\\text{ }\\text{ If } y_t = j \\text{, then add } (x_t, -1) \\text{ to } S_{i,j}$\n",
    "$\\text{ let } h_{i,j} = L(S_{i,j}$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ } (\\Sigma_{j \\in Y} \\text{ sign}(j-i) h_{i,j}(x))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa54a1c-c90e-488b-bdb1-265b147f26f8",
   "metadata": {},
   "source": [
    "### Loss: Logistic Loss + Regularization\n",
    "\n",
    "The loss function of a Logistic Regression classifier over $k$ classes is the **log-loss**, also called **cross-entropy loss**. Since we will only use binary classifier, e.g. Binary Logistic Regression, in this project, only **Binary Log Loss** will be introduced in this section.\n",
    "\n",
    "The Binary Log Loss on a sample of m data points, also called the Binary Cross Entropy Loss, is:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "The corresponding gradient of the Binary Log loss with respect to the model's wights is:\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "We also implement the L2 norm of wights to adpot Tikhonov regularization into our loss function. The L2 norm of wights is:\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "And the gradient of the L2 term with respect to the model's weights is:\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "\n",
    "In conclusion, the total loss function would be:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))+ \\lambda\\sum_{i=1}^{d}w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622a64e-b029-4a79-8568-f9518a31350d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**One-vs-All** and **All-Pairs** are both strategies used to solve muticlass classification problems by utilizing binary classifiers. In this case, Stochastic Gradient Descent (Mini Batch) is a suitable choice.   \n",
    "In gradient descent, the general formula for weight update is:\n",
    "$$w_j = w_j - \\alpha \\cdot \\frac{\\partial L}{\\partial w_j}$$  \n",
    "\n",
    "For each batch of size $m$, the gradient of the binary log loss with respect to the weight is:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij}$$  \n",
    "\n",
    "If incorporate regularization (mentioned in the previous section), the total gradient becomes:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij} + 2 \\lambda w_j$$\n",
    "Thus, the final weight update equation is: \n",
    "$$w_j = w_j - \\alpha \\cdot \\left( \\frac{\\partial L}{\\partial w_j} + 2 \\lambda w_j \\right)$$  \n",
    "\n",
    "Due to the nature of One-vs-All and All-pairs strategies, we apply this optimizer differently compared to direct multiclass classification techniques, such as multiclass logistic regression.  \n",
    "**One-vs-All**: for each class $j$, you train a seperate binary classifier that distinguishes class $j$ from all other classes.  \n",
    "**All-pairs**: for each unique class pair $(i, j)$, you train a binary classifier that differentiates between those two classes.  \n",
    "\n",
    "#### Pseudocode: Stochastic Gradient Descent for Logistic Regression (Lecture 6 Slide 21)  \n",
    "**Inputs**: Traning examples $S$, step size $\\alpha$, batch size $b < |S|$  \n",
    "Set converged false  \n",
    "**while** not converged:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Randomly shuffle $S$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $i = 0$ to $|S|/b - 1$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S'$ = Extracted current batch using $i$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\nabla L_{S'}(h_w)$ + regularization  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;converged = check_convergence$(S,w)$  \n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7202f8-caea-4a1c-b13f-cd9e159d7114",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f803238f-5b7c-4b7f-a506-e471024bd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
    "\n",
    "def get_estimator(train_epochs, lr):\n",
    "    estimator = SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        tol=None,\n",
    "        max_iter=train_epochs,\n",
    "        shuffle=True,\n",
    "        random_state=0,\n",
    "        learning_rate='constant',\n",
    "        eta0=lr,\n",
    "        alpha=0)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24586b5",
   "metadata": {},
   "source": [
    "## Model: Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdb6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StandardScaler:\n",
    "    def _init_(self, X):\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1] # can include bias or not\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "    \n",
    "    def center(self, X):\n",
    "        return X - self.mean\n",
    "    \n",
    "    def scale(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        X_scaled = X_centered/self.std\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94ae8-cdc1-489f-a65f-d210e30a73ae",
   "metadata": {},
   "source": [
    "## **Model: Representation - Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "add411a0-72b0-4f40-965f-2836c707cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    '''\n",
    "    Binary Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, batch_size=1, num_epochs=1, lr=0.0001, tol=1e-4):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.tol = tol\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros((1, num_features))\n",
    "        previous_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            shuffled_inds = np.random.permutation(num_samples)\n",
    "            shuffled_X = X[shuffled_inds]\n",
    "            shuffled_Y = Y[shuffled_inds]\n",
    "\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch = shuffled_X [start: min(end, num_samples)] \n",
    "                Y_batch = shuffled_Y [start: min(end, num_samples)] \n",
    "\n",
    "                predictions = sigmoid(np.dot(X_batch, self.weights.T)) # num_samples * 1 (num_classes)\n",
    "                Y_batch = np.reshape(Y_batch,(len(Y_batch),1)) # num_samples * 1, reshape Y to same dimensions of sigmoid\n",
    "                error = predictions - Y_batch\n",
    "                loss_grad = np.dot(error.T, X_batch)/len(X_batch)\n",
    "    \n",
    "                self.weights -= self.learning_rate * loss_grad\n",
    "            \n",
    "            current_loss = self.loss(X, Y)\n",
    "\n",
    "            # if abs(previous_loss - current_loss) < self.tol:\n",
    "            #     # print(f'Convergence reached at epoch {epoch + 1}')\n",
    "            #     break\n",
    "            \n",
    "\n",
    "            #print(f\"Epoch {epoch + 1}, Weights: {self.weights}\")\n",
    "                            \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Computes the logistic loss (binary cross-entropy loss) for binary classification\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        left_half = Y.T @ np.log(y_pred)\n",
    "        right_half = (1-Y).T @ np.log(1-y_pred)\n",
    "        \n",
    "        # Calculate the logistic loss\n",
    "        loss = -np.mean(left_half + right_half)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # X.shape: (batch size, num features)\n",
    "        # self.weights.shape: (1, num features)\n",
    "        dot_product = np.dot(self.weights, X.T) # n_classes * n_samples\n",
    "        probs = sigmoid(dot_product)\n",
    "        probsall = np.vstack((1-probs, probs)) # probs are for class 2\n",
    "        y_predict = np.argmax(probsall, axis=0) #finding the index of the max value in a column\n",
    "        return y_predict\n",
    "\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predicted_classes = self.predict(X)\n",
    "        return np.mean(predicted_classes == Y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Compute probabilities for the input data X.\n",
    "        @params:\n",
    "        X: A 2D Numpy array where each row contains an example\n",
    "        @return:\n",
    "        Probabilities of each example being in class 1\n",
    "        '''\n",
    "        dot_product = np.dot(self.weights, X.T) # n_classes * n_samples\n",
    "        probs = sigmoid(dot_product)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd5d7-b3ba-4b43-8de9-b00cfcc2ee60",
   "metadata": {},
   "source": [
    "## **Check Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044782d7-d5ee-45ec-bbbb-fdb593d53a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create Test Models\n",
    "# SGDClassifier is always batch_size == 1\n",
    "train_epochs = 1\n",
    "lr = 0.01\n",
    "my_model = MyLogisticRegression(lr=lr, batch_size=1, num_epochs=train_epochs)\n",
    "sklearn_model = SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=train_epochs,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    learning_rate='constant',\n",
    "    eta0=lr,\n",
    "    alpha=0)\n",
    "\n",
    "# Creates Test Data\n",
    "x = np.array([[0,4], [0,3], [5,0], [4,1], [0,5]])\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "\n",
    "my_model.train(x_bias, y)\n",
    "sklearn_model.fit(x, y) # Gives same result as partial_fit\n",
    "\n",
    "weights = my_model.weights\n",
    "assert isinstance(weights, np.ndarray)\n",
    "assert weights.ndim==2 and weights.shape == (1,3)\n",
    "# FIXME: relative tolerance might be not strict enough\n",
    "assert weights[0][:-1] == pytest.approx(sklearn_model.coef_[0], 0.01)\n",
    "assert weights[0][-1] == pytest.approx(sklearn_model.intercept_[0], 0.01)\n",
    "\n",
    "# print('My Model Weights',my_model.weights)\n",
    "# print('sklearn weights',sklearn_model.coef_[0])\n",
    "# print('sklearn bias', sklearn_model.intercept_)\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "train_epochs = 10\n",
    "lr = 0.01\n",
    "my_model = MyLogisticRegression(lr=lr, batch_size=1, num_epochs=train_epochs)\n",
    "sklearn_model = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    tol=None,\n",
    "    max_iter=train_epochs,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    learning_rate='constant',\n",
    "    eta0=lr,\n",
    "    alpha=0)\n",
    "\n",
    "my_model.train(x_bias, y)\n",
    "sklearn_model.fit(x, y, coef_init=[[0,0]], intercept_init=[0])\n",
    "\n",
    "weights = my_model.weights\n",
    "# print('My Model Weights',my_model.weights)\n",
    "# print('sklearn weights',sklearn_model.coef_[0])\n",
    "# print('sklearn bias', sklearn_model.intercept_)\n",
    "assert weights[0][:-1] == pytest.approx(sklearn_model.coef_[0], 0.01)\n",
    "assert weights[0][-1] == pytest.approx(sklearn_model.intercept_[0], 0.1)\n",
    "\n",
    "# Test model predictions\n",
    "x_test = np.array([[0,0], [-5,3], [9,0], [1,0], [6,-7]])\n",
    "x_test_bias = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "my_preds = my_model.predict(x_test_bias)\n",
    "sklearn_preds = sklearn_model.predict(x_test)\n",
    "assert (my_preds == sklearn_preds).all()\n",
    "\n",
    "print(\"Check Model Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04b184-3367-412a-85d9-66a079afc561",
   "metadata": {},
   "source": [
    "## **Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1aa45-ab5f-4556-a4b1-baee46a7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnevsAll:\n",
    "    def __init__(self, n_classes, batch_size=1, epochs=1, lr=0.01):\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        # self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        num_inputs = X.shape[0]\n",
    "        num_features = X.shape[1] - 1 # there's bias\n",
    "        \n",
    "        # Split data and train each representation\n",
    "        self.S_Y = np.array([np.array(Y) for _ in range(self.n_classes)])\n",
    "        self.h = np.array([\n",
    "            MyLogisticRegression(\n",
    "                batch_size=self.batch_size, \n",
    "                num_epochs=self.epochs, \n",
    "                lr=self.lr, \n",
    "                tol=0) for _ in range(self.n_classes)])\n",
    "        self.conv_epochs = [0] * self.n_classes\n",
    "        for cls in range(self.n_classes):\n",
    "            # Create S_i for each class i\n",
    "            S_Y_i = self.S_Y[cls]\n",
    "            cls_idx = S_Y_i == cls\n",
    "            S_Y_i[cls_idx] = 1\n",
    "            non_cls_idx = np.logical_not(cls_idx)\n",
    "            S_Y_i[non_cls_idx] = 0\n",
    "            \n",
    "            # Train h_i for each class i on S_i\n",
    "            h_i = self.h[cls]\n",
    "            conv_epoch = h_i.train(X, S_Y_i)\n",
    "            self.conv_epochs.append(conv_epoch)\n",
    "            \n",
    "    # def loss(self, X, Y):\n",
    "    #     preds = self.predict(X)\n",
    "    #     # L1-loss\n",
    "    #     losses = np.abs(Y-preds)\n",
    "    #     losses = np.sum(losses)\n",
    "    #     return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # h_i in argmax h_i(x)\n",
    "        predictions = [0] * X.shape[0]\n",
    "        for i, x in enumerate(X):\n",
    "            preds = [0] * self.n_classes\n",
    "            # Get predictions from all hypotheses\n",
    "            for c in range(self.n_classes):\n",
    "                preds[c] = self.h[c].predict_proba(x)\n",
    "            # Select max prediction\n",
    "            predictions[i] = np.argmax(preds)\n",
    "            # print(preds, predictions[i])\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, preds, Y):\n",
    "        correct_preds = Y == preds\n",
    "        return np.sum(correct_preds) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc0487-1397-47de-a8e8-5b5a9bb18e08",
   "metadata": {},
   "source": [
    "## **Check Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523621b-f962-4448-8281-04e629807402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "np.random.seed(0)\n",
    "\n",
    "# Get data for testing\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "bias_col = np.ones((X.shape[0], 1))\n",
    "X_bias = np.hstack((X, bias_col))\n",
    "\n",
    "Y = data.target\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "# Initialize models:\n",
    "train_epochs = 5\n",
    "lr = 0.03\n",
    "my_model = OnevsAll(n_classes)\n",
    "\n",
    "estimator = get_estimator(train_epochs, lr)\n",
    "sklearn_model = OneVsRestClassifier(estimator)\n",
    "\n",
    "my_model.train(X_bias, Y)\n",
    "sklearn_model.fit(X, Y)\n",
    "\n",
    "# Check that S is populated correctly\n",
    "assert my_model.S_Y.shape[0] == n_classes\n",
    "assert my_model.S_Y.shape[1] == Y.shape[0]\n",
    "\n",
    "# Check h\n",
    "assert len(my_model.h) == n_classes\n",
    "assert len(sklearn_model.estimators_) == n_classes\n",
    "for h in my_model.h:\n",
    "    assert isinstance(h, MyLogisticRegression)\n",
    "for i in range(n_classes):\n",
    "    my_weights = my_model.h[i].weights[0][:-1]\n",
    "    my_bias = my_model.h[i].weights[0][-1]\n",
    "    sklearn_weights = sklearn_model.estimators_[i].coef_[0]\n",
    "    sklearn_bias = sklearn_model.estimators_[i].intercept_[0]\n",
    "    print(\" === \", i)\n",
    "    print(my_weights)\n",
    "    print(sklearn_weights)\n",
    "    print(my_bias)\n",
    "    print(sklearn_bias)\n",
    "\n",
    "    # assert my_weights == pytest.approx(sklearn_weights, 0.01)\n",
    "    # assert my_bias == pytest.approx(sklearn_bias, 0.01)\n",
    "\n",
    "# Check predictions\n",
    "predictions = my_model.predict(X_bias)\n",
    "print(\"My Predictions:\", np.array(predictions))\n",
    "\n",
    "sklearn_predictions = sklearn_model.predict(X)\n",
    "print(\"sklearn Predictions:\", sklearn_predictions)\n",
    "\n",
    "print('num_samples', X.shape[0])\n",
    "print('Differences', np.sum(np.abs(np.array(predictions)-sklearn_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb198-b61a-4b1c-83f1-cfd3b41c4d8c",
   "metadata": {},
   "source": [
    "## **Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a429170-f86d-4cd0-bf28-a31c9342d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can only use python and numpy in this section.\n",
    "import numpy as np\n",
    "\n",
    "class AllPairs:\n",
    "    def __init__(self, n_classes, conv_threshold, batch_size, epochs, lr):\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.models = {}\n",
    "        self.lr = lr\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(i + 1, self.n_classes):\n",
    "                selected_indices = []\n",
    "                for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "                X_selected = X[selected_indices]\n",
    "                Y_selected = Y[selected_indices]\n",
    "                \n",
    "                for idx in range(len(Y_selected)):\n",
    "                    if Y_selected[idx] == i:\n",
    "                        Y_selected[idx] = 0\n",
    "                    else:\n",
    "                        Y_selected[idx] = 1\n",
    "\n",
    "                model = MyLogisticRegression(batch_size=self.batch_size, num_epochs=self.epochs, lr=self.lr, tol=self.conv_threshold)\n",
    "                model.train(X_selected, Y_selected)\n",
    "                key = i, j\n",
    "                self.models[key] = model\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Average Logistic loss?\n",
    "        total_loss = 0\n",
    "        for (i, j), model in self.models.items():\n",
    "            selected_indices = []\n",
    "            for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "            X_selected = X[selected_indices]\n",
    "            Y_selected = Y[selected_indices]\n",
    "            \n",
    "            Y_selected = np.where(Y_selected == i, 0, 1)\n",
    "            total_loss += model.loss(X_selected, Y_selected)\n",
    "\n",
    "        return total_loss / len(self.models)\n",
    "        \n",
    "        '''\n",
    "        prediction = self.predict(X)\n",
    "        losses = np.abs(Y - prediction)\n",
    "        return np.sum(losses)\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], self.n_classes))\n",
    "        for key in self.models:\n",
    "            i, j = key\n",
    "            model = self.models[key]\n",
    "            prediction = model.predict(X)\n",
    "    \n",
    "            for idx in range(len(prediction)):\n",
    "                if prediction[idx] == 0:\n",
    "                    votes[idx, i] = votes[idx, i] + 1 \n",
    "                elif prediction[idx] == 1:\n",
    "                    votes[idx, j] = votes[idx, j] + 1\n",
    "        return np.argmax(votes, axis=1)\n",
    "        \n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe72db-d6e7-44e2-ade1-991a5658a1b5",
   "metadata": {},
   "source": [
    "## **Check Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18136c5-fe5d-4fc6-9436-7988a32a13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "bias_col = np.ones((X.shape[0], 1))\n",
    "X_biased = np.hstack((X, bias_col))\n",
    "\n",
    "Y = data.target\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "model = AllPairs(n_classes=n_classes, conv_threshold=0.001, batch_size=10, epochs=5000, lr=0.01)\n",
    "model.train(X_biased, Y)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(X_biased)\n",
    "\n",
    "\n",
    "print(\"PredictionsA:\", np.array(predictions))\n",
    "\n",
    "sklearn_predictions = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(X)\n",
    "print(\"PredictionsB:\", sklearn_predictions)\n",
    "\n",
    "print('num_samples', X.shape[0])\n",
    "print('Differences', np.sum(np.abs(np.array(predictions)-sklearn_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba57367-f899-45ba-aef9-7cc72db1d923",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3badda3-9165-45b5-9697-f5ce9e3adcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier # Use this instead of LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.data.shape)  # (150, 4)\n",
    "print(iris.feature_names)  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "print(iris.target_names)  # ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "num_classes = len(iris.target_names)\n",
    "\n",
    "# Compare performance with sklearn on iris dataset\n",
    "random_seed = 0\n",
    "train_epochs = 1\n",
    "one_estimators = [SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=train_epochs, \n",
    "    shuffle=True,\n",
    "    random_state=random_seed,\n",
    "    learning_rate='constant') for _ in range(num_classes)]\n",
    "one_model = OneVsOneClassifier(one_estimators)\n",
    "\n",
    "rest_estimators = [SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=train_epochs, \n",
    "    shuffle=True,\n",
    "    random_state=random_seed,\n",
    "    learning_rate='constant') for _ in range(num_classes)]\n",
    "rest_model = OneVsRestClassifier(rest_estimators)\n",
    "\n",
    "# Note: SGDClassifier.partial_fit does not allow specific batch size.\n",
    "# SGDClassifier trains on batch size == 1 in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6248542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "# Apply PCA to reduce the dimensions to 2 for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12aa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your One-vs-All model\n",
    "'''one_vs_all_model = OnevsAll(n_classes=len(np.unique(Y)))\n",
    "one_vs_all_model.train(X_reduced, Y)'''\n",
    "\n",
    "# Train your All-Pairs model\n",
    "n_classes=len(np.unique(Y))\n",
    "all_pairs_model = AllPairs(n_classes=n_classes, batch_size=10, conv_threshold=0.001, epochs=5000, lr=0.01)\n",
    "all_pairs_model.train(X_reduced, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X, Y, model, title, subplot_position):\n",
    "    # Create a meshgrid of points\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "\n",
    "    # Predict for each point in the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotting decision boundaries\n",
    "    plt.subplot(subplot_position)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolor='k', s=30, cmap=plt.cm.Set1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "\n",
    "# Prepare figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot decision boundary for your One-vs-All model\n",
    "'''plot_decision_boundaries(X_reduced, Y, one_vs_all_model, \"One-vs-All Model\", 222)'''\n",
    "\n",
    "# Plot decision boundary for your All-Pairs model\n",
    "plot_decision_boundaries(X_reduced, Y, all_pairs_model, \"All-Pairs Model\", 223)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
