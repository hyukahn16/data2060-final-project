{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105f1e4-612d-4c5a-a890-66c26f9cf031",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification: one-vs-all and all-pairs**\n",
    "\n",
    "Give an overview of the algorithm and describe its advantages and disadvantages.\n",
    "\n",
    "#### <ins>One-vs-All\n",
    "\n",
    "This algorithm creates a classifier for each class (3 classifiers if there exists 3 classes). For each classifier, it is responsible for predicting whether an input belongs to its corresponding class or not.\n",
    "\n",
    "Each classifier is trained on the entire dataset with modifications corresponding to each classifier.\\\n",
    "The modification changes the dataset such that when you're training a classifier for class $3$, labels for all the other classes are modified to $-1$ and labels for the classifier's class are modified to $1$. (More details will be provided in the Representation section)\n",
    "\n",
    "#### <ins>All-Pairs\n",
    "\n",
    "This algorithm creates a classifier for each pair of classes. For each classifier, it is responsible for predicting whether a given input belongs to one class or the other.\n",
    "\n",
    "Each classifier is trained a portion of the dataset with modifications corresponding to each classifier.\\\n",
    "First, each classifier is assigned portion of the dataset that contains the classes the classifier is predicting for. Then, the assigned data's classes are changed so that one class is assigned the label of $1$ and the other is assigned the label of $-1$.\n",
    "\n",
    "#### <ins>Advantages and Disadvantages of Multiclass Classification\n",
    "\n",
    "Multiclass classification algorithm is an algorithm that classifies an input that can belong to one of the multiple classes (more than two classes).\\\n",
    "For this project, we will be implementing One-vs-All and All-Pairs algorithms for the multiclass classification of the UCI Iris dataset.\n",
    "\n",
    "Compared to a multiclass classification algorithm that inherently encompasses multiclass classification (output of model predicts multiclass),\n",
    "the main advantages of One-vs-All and All-Pairs stems from the use of binary classifiers.\\\n",
    "Because of the binary classifiers to represent multiclass classification, these two algorithms have implementation simplicity and easy interpretability of the predictions.\n",
    "\n",
    "Unfortunately, the disadvantages also stem from the use of binary classifiers.\n",
    "- The binary classifiers do not have any knowledge that it is used for multiclass classification and therefore, does not have inherent understanding of the multiclass classification problem.\n",
    "- Due to training classifier for each class, each classifier is trained on a class imbalanced dataset and may result in overfitting.\n",
    "- Training multiple classifiers can be computationally expensive.\n",
    "\n",
    "#### <ins>Misc.\n",
    "\n",
    "In this final project, we will be using the UCI Iris dataset we encountered in our previous homework:\\\n",
    "[`https://archive.ics.uci.edu/dataset/53/iris`](https://archive.ics.uci.edu/dataset/53/iris)\n",
    "\n",
    "What we will be comparing to:\n",
    "[scikit-learn multiclass classification](https://scikit-learn.org/1.5/modules/multiclass.html#multilabel-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ba03-9fb9-49ce-8201-a319beb7f36f",
   "metadata": {},
   "source": [
    "### Representation: Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Given sample's feature values $x \\in \\mathbb{R}^{d}$ and a label $y  \\in \\{0, 1\\}$, binary classification of input $x$ is predicted through combination of affine function and sigmoid function.\n",
    "$$ y = \\langle w, x\\rangle $$ \n",
    "$$\\sigma (y) = \\frac{1}{1 + e^{-y}}$$\n",
    "Therefore, our hypothesis function defined on weights $w$ is\n",
    "$$h_{w}(x) = \\frac{1}{1 + e^{-\\langle w, x\\rangle}}$$\n",
    "\n",
    "#### Multiclass Logistic Regression\n",
    "Now, using the binary logistic regression defined above, we will define one-vs-all and all-pairs multiclass logistic regression algorithms.\n",
    "\n",
    "Pseudocode for one-vs-all (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i \\in Y:$\\\n",
    "$\\text{ let } S_i = (x_1, (-1)^{\\mathbb{1}_{[y_1 \\neq i]}}), ..., (x_m, (-1)^{\\mathbb{1}_{[y_m \\neq i]}})$\\\n",
    "$\\text{ let } h_i = L(S_i)$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ }h_i(x)$\n",
    "\n",
    "\n",
    "Pseudocode for all-pairs (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i,j \\in Y \\text{ such that } i < j$\\\n",
    "$\\text{ initialize empty } S_{i,j}$\\\n",
    "$\\text{ for } t = 1, ..., m$\\\n",
    "$\\text{ }\\text{ If } y_t = i \\text{, then add } (x_t, 1) \\text{ to } S_{i,j}$\n",
    "$\\text{ }\\text{ If } y_t = j \\text{, then add } (x_t, -1) \\text{ to } S_{i,j}$\n",
    "$\\text{ let } h_{i,j} = L(S_{i,j}$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ } (\\Sigma_{j \\in Y} \\text{ sign}(j-i) h_{i,j}(x))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa54a1c-c90e-488b-bdb1-265b147f26f8",
   "metadata": {},
   "source": [
    "### Loss: Logistic Loss + Regularization\n",
    "\n",
    "The loss function of a Logistic Regression classifier over $k$ classes is the **log-loss**, also called **cross-entropy loss**. Since we will only use binary classifier, e.g. Binary Logistic Regression, in this project, only **Binary Log Loss** will be introduced in this section.\n",
    "\n",
    "The Binary Log Loss on a sample of m data points, also called the Binary Cross Entropy Loss, is:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "The corresponding gradient of the Binary Log loss with respect to the model's wights is:\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "We also implement the L2 norm of wights to adpot Tikhonov regularization into our loss function. The L2 norm of wights is:\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "And the gradient of the L2 term with respect to the model's weights is:\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "\n",
    "In conclusion, the total loss function would be:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))+ \\lambda\\sum_{i=1}^{d}w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622a64e-b029-4a79-8568-f9518a31350d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**One-vs-All** and **All-Pairs** are both strategies used to solve muticlass classification problems by utilizing binary classifiers. In this case, Stochastic Gradient Descent (Mini Batch) is a suitable choice.   \n",
    "In gradient descent, the general formula for weight update is:\n",
    "$$w_j = w_j - \\alpha \\cdot \\frac{\\partial L}{\\partial w_j}$$  \n",
    "\n",
    "For each batch of size $m$, the gradient of the binary log loss with respect to the weight is:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij}$$  \n",
    "\n",
    "If incorporate regularization (mentioned in the previous section), the total gradient becomes:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij} + 2 \\lambda w_j$$\n",
    "Thus, the final weight update equation is: \n",
    "$$w_j = w_j - \\alpha \\cdot \\left( \\frac{\\partial L}{\\partial w_j} + 2 \\lambda w_j \\right)$$  \n",
    "\n",
    "Due to the nature of One-vs-All and All-pairs strategies, we apply this optimizer differently compared to direct multiclass classification techniques, such as multiclass logistic regression.  \n",
    "**One-vs-All**: for each class $j$, you train a seperate binary classifier that distinguishes class $j$ from all other classes.  \n",
    "**All-pairs**: for each unique class pair $(i, j)$, you train a binary classifier that differentiates between those two classes.  \n",
    "\n",
    "#### Pseudocode: Stochastic Gradient Descent for Logistic Regression (Lecture 6 Slide 21)  \n",
    "**Inputs**: Traning examples $S$, step size $\\alpha$, batch size $b < |S|$  \n",
    "Set converged false  \n",
    "**while** not converged:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Randomly shuffle $S$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $i = 0$ to $|S|/b - 1$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S'$ = Extracted current batch using $i$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\nabla L_{S'}(h_w)$ + regularization  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;converged = check_convergence$(S,w)$  \n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7202f8-caea-4a1c-b13f-cd9e159d7114",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f803238f-5b7c-4b7f-a506-e471024bd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94ae8-cdc1-489f-a65f-d210e30a73ae",
   "metadata": {},
   "source": [
    "## **Model: Representation - Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add411a0-72b0-4f40-965f-2836c707cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    '''\n",
    "    Binary Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, batch_size=1, num_epochs=1, lr=0.0001):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        # self.lmbda = 1 # tune this parameter\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros((1, num_features))\n",
    "        for e in range(self.num_epochs):\n",
    "            num_batches = int(np.ceil(X.shape[0]/self.batch_size))\n",
    "            shuffled_inds = np.random.permutation(X.shape[0])\n",
    "            shuffled_X = X[shuffled_inds]\n",
    "            shuffled_Y = Y[shuffled_inds]\n",
    "            for i in range(0, num_batches, 1):\n",
    "                X_batch = shuffled_X [i:min(i+self.batch_size, num_samples)] \n",
    "                Y_batch = shuffled_Y[i:min(i+self.batch_size, num_samples)]\n",
    "                loss_grad = np.zeros(self.weights.shape)\n",
    "\n",
    "                wdotx = np.dot(X_batch, self.weights.T) # num_samples * 1 (num_classes)\n",
    "                probs = sigmoid(wdotx) # num_samples * 1 (num_classes)\n",
    "                Y_batch = np.reshape(Y_batch,(len(Y_batch),1)) # num_samples * 1, reshape Y to same dimensions of sigmoid\n",
    "                loss_grad = np.sum((probs-Y_batch)*X_batch, axis = 0)/len(X_batch)\n",
    "                loss_grad = np.reshape(loss_grad,(1,len(loss_grad)))\n",
    "\n",
    "                self.weights -= self.learning_rate * loss_grad\n",
    "                # weight_decay_grad = 2 * self.lmbda * self.weights\n",
    "                # self.weights -= self.learning_rate * (loss_grad + weight_decay_grad)\n",
    "                            \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Computes the logistic loss (binary cross-entropy loss) for binary classification\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        left_half = Y.T @ np.log(y_pred)\n",
    "        right_half = (1-Y).T @ np.log(1-y_pred)\n",
    "        \n",
    "        # Calculate the logistic loss\n",
    "        loss = -np.mean(left_half + right_half)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # X.shape: (batch size, num features)\n",
    "        # self.weights.shape: (1, num features)\n",
    "        dot_product = np.dot(self.weights, X.T) # n_classes * n_samples\n",
    "        probs = sigmoid(dot_product)\n",
    "        probsall = np.vstack((1-probs, probs)) # probs are for class 2\n",
    "        y_predict = np.argmax(probsall, axis=0) #finding the index of the max value in a column\n",
    "        return y_predict\n",
    "\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predicted_classes = self.predict(X)\n",
    "        return np.mean(predicted_classes == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd5d7-b3ba-4b43-8de9-b00cfcc2ee60",
   "metadata": {},
   "source": [
    "## **Check Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc03340",
   "metadata": {},
   "source": [
    "#### tests taken from HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2d133f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression.__init__() got multiple values for argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Creates Test Models\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m test_model1 \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m test_model2 \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Creates Test Data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: LogisticRegression.__init__() got multiple values for argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = LogisticRegression(3, batch_size=15, num_epochs=10000, lr=0.00001)\n",
    "test_model2 = LogisticRegression(3, batch_size=15, num_epochs=10000, lr=0.00001)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,-3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,0,1,0])\n",
    "\n",
    "\n",
    "# Test Train Model and Checks Model Weights\n",
    "test_model1.train(x_bias, y)\n",
    "weights1 = test_model1.weights\n",
    "assert isinstance(weights1, np.ndarray)\n",
    "assert weights1.ndim==2 and weights1.shape == (1,3)\n",
    "assert weights1 == pytest.approx(np.array([[0.15226533, -0.17509754, -0.01464057]]), 0.05)\n",
    "\n",
    "test_model2.train(x_bias2, y2)\n",
    "weights2 = test_model2.weights\n",
    "assert isinstance(weights2, np.ndarray)\n",
    "assert weights2.ndim==2 and weights2.shape == (1,3)\n",
    "assert weights2 == pytest.approx(np.array([[0.13295026, 0.10046715, 0.02296362]]), 0.05)\n",
    "\n",
    "# Test Model Predict\n",
    "predict1 = test_model1.predict(x_bias_test)\n",
    "assert isinstance(predict1, np.ndarray)\n",
    "assert predict1.ndim==1 and predict1.shape==(5,)\n",
    "assert (predict1 == np.array([0, 0, 1, 1, 1])).all()\n",
    "\n",
    "predict2 = test_model2.predict(x_bias_test2)\n",
    "assert isinstance(predict2, np.ndarray)\n",
    "assert predict2.ndim==1 and predict2.shape==(4,)\n",
    "assert (test_model2.predict(x_bias_test2) == np.array([1, 0, 1, 1])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "accuracy1 = test_model1.accuracy(x_bias_test, y_test)\n",
    "assert isinstance(accuracy1, float)\n",
    "assert accuracy1 == .8\n",
    "\n",
    "accuracy2 = test_model2.accuracy(x_bias_test2, y_test2)\n",
    "assert isinstance(accuracy2, float)\n",
    "assert accuracy2 == .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044782d7-d5ee-45ec-bbbb-fdb593d53a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.009 -0.035 -0.007]] [[ 0.04449232 -0.05330186 -0.00461729]]\n",
      "[-0.00461729]\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "lr = 0.01\n",
    "my_model = LogisticRegression(lr=lr, batch_size=5, num_epochs=1)\n",
    "test_model = SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=1, \n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    learning_rate='constant',\n",
    "    eta0=lr)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([-1,-1,1,1,-1])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([-1,-1,1,-1,1])\n",
    "\n",
    "my_model.train(x_bias, y)\n",
    "test_model.partial_fit(x_bias, y, classes=[-1, 1])\n",
    "\n",
    "print(my_model.weights, test_model.coef_)\n",
    "print(test_model.intercept_)\n",
    "\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,-3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,0,1,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04b184-3367-412a-85d9-66a079afc561",
   "metadata": {},
   "source": [
    "## **Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce1aa45-ab5f-4556-a4b1-baee46a7c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can only use python and numpy in this section.\n",
    "import numpy as np\n",
    "\n",
    "class OnevsAll:\n",
    "    def __init__(self, n_classes, conv_threshold, batch_size=1, epochs=1):\n",
    "        self.n_classes = n_classes\n",
    "        self.alpha = 0.03\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        num_inputs = X.shape[0]\n",
    "        num_features = X.shape[1] - 1 # there's bias\n",
    "        \n",
    "        # Split data and train each representation\n",
    "        self.S_Y = np.array([np.array(Y) for _ in range(self.n_classes)])\n",
    "        self.h = np.array([\n",
    "            LogisticRegression(num_features, self.n_classes, self.batch_size, self.conv_threshold)] * self.n_classes)\n",
    "        self.conv_epochs = [0] * self.n_classes\n",
    "        for cls in range(self.n_classes):\n",
    "            # Create S_i for each class i\n",
    "            S_Y_i = self.S_Y[cls]\n",
    "            cls_idx = S_Y_i == cls\n",
    "            S_Y_i[cls_idx] = 1\n",
    "            non_cls_idx = np.logical_not(cls_idx)\n",
    "            S_Y_i[non_cls_idx] = -1\n",
    "            \n",
    "            # Train h_i for each class i on S_i\n",
    "            h_i = self.h[cls]\n",
    "            conv_epoch = h_i.train(X, S_Y_i)\n",
    "            self.conv_epochs.append(conv_epoch)\n",
    "            \n",
    "    def loss(self, X, Y):\n",
    "        preds = self.predict(X)\n",
    "        # L1-loss\n",
    "        losses = np.abs(Y-preds)\n",
    "        losses = np.sum(losses)\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # h_i in argmax h_i(x)\n",
    "        predictions = [0] * X.shape[0]\n",
    "        for i, x in enumerate(X):\n",
    "            preds = [0] * self.n_classes\n",
    "            # Get predictions from all hypotheses\n",
    "            for c in self.n_classes:\n",
    "                preds[c] = self.h[c](x)\n",
    "            # Select max prediction\n",
    "            predictions[i] = np.argmax(preds)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, preds, Y):\n",
    "        correct_preds = Y == preds\n",
    "        return np.sum(correct_preds) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc0487-1397-47de-a8e8-5b5a9bb18e08",
   "metadata": {},
   "source": [
    "## **Check Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b523621b-f962-4448-8281-04e629807402",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression.__init__() takes from 1 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m ovaModel \u001b[38;5;241m=\u001b[39m OnevsAll(n_classes, conv_threshold)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check that S is populated correctly\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43movaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ovaModel\u001b[38;5;241m.\u001b[39mS_Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n_classes\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ovaModel\u001b[38;5;241m.\u001b[39mS_Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mOnevsAll.train\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Split data and train each representation\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS_Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(Y) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes)])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_threshold\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_epochs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Create S_i for each class i\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: LogisticRegression.__init__() takes from 1 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_bias = np.array([[1,1,1,1,1]])\n",
    "Y = np.array([2])\n",
    "\n",
    "n_classes = 3\n",
    "conv_threshold = 0.5\n",
    "ovaModel = OnevsAll(n_classes, conv_threshold)\n",
    "\n",
    "# Check that S is populated correctly\n",
    "ovaModel.train(X_bias, Y)\n",
    "assert ovaModel.S_Y.shape[0] == n_classes\n",
    "assert ovaModel.S_Y.shape[1] == Y.shape[0]\n",
    "\n",
    "# Check h\n",
    "assert len(ovaModel.h) == n_classes\n",
    "for h in ovaModel.h:\n",
    "    assert isinstance(h, LogisticRegression)\n",
    "\n",
    "# Check predictions\n",
    "\n",
    "\n",
    "# Check accuracy\n",
    "\n",
    "# Check loss\n",
    "\n",
    "\n",
    "print(\"Check Model Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcb198-b61a-4b1c-83f1-cfd3b41c4d8c",
   "metadata": {},
   "source": [
    "## **Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a429170-f86d-4cd0-bf28-a31c9342d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can only use python and numpy in this section.\n",
    "import numpy as np\n",
    "\n",
    "class AllPairs:\n",
    "    def __init__(self, n_classes, conv_threshold, batch_size=1, epochs=1):\n",
    "        self.n_classes = n_classes\n",
    "        self.alpha = 0.03\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.models = {}\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(i + 1, self.n_classes):\n",
    "                selected_indices = []\n",
    "                for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "                X_selected = X[selected_indices]\n",
    "                Y_selected = Y[selected_indices]\n",
    "                \n",
    "                for idx in range(len(Y_selected)):\n",
    "                    if Y_selected[idx] == i:\n",
    "                        Y_selected[idx] = 0\n",
    "                    else:\n",
    "                        Y_selected[idx] = 1\n",
    "\n",
    "                model = LogisticRegression(self.batch_size, self.epochs, self.conv_threshold)\n",
    "                model.train(X_selected, Y_selected)\n",
    "                key = i, j\n",
    "                self.models[key] = model\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Average Logistic loss?\n",
    "        total_loss = 0\n",
    "        for (i, j), model in self.models.items():\n",
    "            selected_indices = []\n",
    "            for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "            X_selected = X[selected_indices]\n",
    "            Y_selected = Y[selected_indices]\n",
    "            \n",
    "            Y_selected = np.where(Y_selected == i, 0, 1)\n",
    "            total_loss += model.loss(X_selected, Y_selected)\n",
    "\n",
    "        return total_loss / len(self.models)\n",
    "        \n",
    "        '''\n",
    "        prediction = self.predict(X)\n",
    "        losses = np.abs(Y - prediction)\n",
    "        return np.sum(losses)\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], self.n_classes))\n",
    "        for key in self.models:\n",
    "            i, j = key\n",
    "            model = self.models[key]\n",
    "            prediction = model.predict(X)\n",
    "    \n",
    "            for idx in range(len(prediction)):\n",
    "                if prediction[idx] == 0:\n",
    "                    votes[idx, i] = votes[idx, i] + 1 \n",
    "                elif prediction[idx] == 1:\n",
    "                    votes[idx, j] = votes[idx, j] + 1\n",
    "        return np.argmax(votes, axis=1)\n",
    "        \n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe72db-d6e7-44e2-ade1-991a5658a1b5",
   "metadata": {},
   "source": [
    "## **Check Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b18136c5-fe5d-4fc6-9436-7988a32a13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "PredictionsA: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "PredictionsB: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "np.random.seed(0)\n",
    "print(\"begin\")\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "all_pairs = AllPairs(n_classes=n_classes, conv_threshold=0.01, batch_size=20, epochs=200)\n",
    "all_pairs.train(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "predictions = all_pairs.predict(X)\n",
    "\n",
    "\n",
    "print(\"PredictionsA:\", np.array(predictions))\n",
    "\n",
    "sklearn_predictions = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(X)\n",
    "print(\"PredictionsB:\", sklearn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba57367-f899-45ba-aef9-7cc72db1d923",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3badda3-9165-45b5-9697-f5ce9e3adcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier # Use this instead of LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.data.shape)  # (150, 4)\n",
    "print(iris.feature_names)  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "print(iris.target_names)  # ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "num_classes = len(iris.target_names)\n",
    "\n",
    "# Compare performance with sklearn on iris dataset\n",
    "random_seed = 0\n",
    "train_epochs = 1\n",
    "one_estimators = [SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=train_epochs, \n",
    "    shuffle=True,\n",
    "    random_state=random_seed,\n",
    "    learning_rate='constant') for _ in range(num_classes)]\n",
    "one_model = OneVsOneClassifier(one_estimators)\n",
    "\n",
    "rest_estimators = [SGDClassifier(\n",
    "    loss='log_loss', \n",
    "    max_iter=train_epochs, \n",
    "    shuffle=True,\n",
    "    random_state=random_seed,\n",
    "    learning_rate='constant') for _ in range(num_classes)]\n",
    "rest_model = OneVsRestClassifier(rest_estimators)\n",
    "\n",
    "# Use partial_fit to train SGDClassifier with specific batch size?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
