{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105f1e4-612d-4c5a-a890-66c26f9cf031",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification: one-vs-all and all-pairs**\n",
    "\n",
    "Give an overview of the algorithm and describe its advantages and disadvantages.\n",
    "\n",
    "#### <ins>One-vs-All\n",
    "\n",
    "This algorithm creates a classifier for each class (3 classifiers if there exists 3 classes). For each classifier, it is responsible for predicting whether an input belongs to its corresponding class or not.\n",
    "\n",
    "Each classifier is trained on the entire dataset with modifications corresponding to each classifier.\\\n",
    "The modification changes the dataset such that when you're training a classifier for class $3$, labels for all the other classes are modified to $0$ and labels for the classifier's class are modified to $1$. (More details will be provided in the Representation section)\n",
    "\n",
    "#### <ins>All-Pairs\n",
    "\n",
    "This algorithm creates a classifier for each pair of classes. For each classifier, it is responsible for predicting whether a given input belongs to one class or the other.\n",
    "\n",
    "Each classifier is trained a portion of the dataset with modifications corresponding to each classifier.\\\n",
    "First, each classifier is assigned portion of the dataset that contains the classes the classifier is predicting for. Then, the assigned data's classes are changed so that one class is assigned the label of $1$ and the other is assigned the label of $-1$.\n",
    "\n",
    "#### <ins>Advantages and Disadvantages of Multiclass Classification\n",
    "\n",
    "Multiclass classification algorithm is an algorithm that classifies an input that can belong to one of the multiple classes (more than two classes).\\\n",
    "For this project, we will be implementing One-vs-All and All-Pairs algorithms for the multiclass classification of the UCI Iris dataset.\n",
    "\n",
    "Compared to a multiclass classification algorithm that inherently encompasses multiclass classification (output of model predicts multiclass),\n",
    "the main advantages of One-vs-All and All-Pairs stems from the use of binary classifiers.\\\n",
    "Because of the binary classifiers to represent multiclass classification, these two algorithms have implementation simplicity and easy interpretability of the predictions.\n",
    "\n",
    "Unfortunately, the disadvantages also stem from the use of binary classifiers.\n",
    "- The binary classifiers do not have any knowledge that it is used for multiclass classification and therefore, does not have inherent understanding of the multiclass classification problem.\n",
    "- Due to training classifier for each class, each classifier is trained on a class imbalanced dataset and may result in overfitting.\n",
    "- Training multiple classifiers can be computationally expensive.\n",
    "\n",
    "#### <ins>Misc.\n",
    "\n",
    "In this final project, we will be using the UCI Iris dataset we encountered in our previous homework:\\\n",
    "[`https://archive.ics.uci.edu/dataset/53/iris`](https://archive.ics.uci.edu/dataset/53/iris)\n",
    "\n",
    "What we will be comparing to:\n",
    "[scikit-learn multiclass classification](https://scikit-learn.org/1.5/modules/multiclass.html#multiclass-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ba03-9fb9-49ce-8201-a319beb7f36f",
   "metadata": {},
   "source": [
    "### Representation: Logistic Regression\n",
    "\n",
    "#### Binary Logistic Regression\n",
    "Given sample's feature values $x \\in \\mathbb{R}^{d}$ and a label $y  \\in \\{0, 1\\}$, binary classification of input $x$ is predicted through combination of affine function and sigmoid function.\n",
    "$$ y = \\langle w, x\\rangle $$ \n",
    "$$\\sigma (y) = \\frac{1}{1 + e^{-y}}$$\n",
    "Therefore, our hypothesis function defined on weights $w$ is\n",
    "$$h_{w}(x) = \\frac{1}{1 + e^{-\\langle w, x\\rangle}}$$\n",
    "\n",
    "#### Multiclass Logistic Regression\n",
    "Now, using the binary logistic regression defined above, we will define one-vs-all and all-pairs multiclass logistic regression algorithms.\n",
    "\n",
    "Pseudocode for one-vs-all (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i \\in Y:$\\\n",
    "$\\text{ let } S_i = (x_1, (-1)^{\\mathbb{1}_{[y_1 \\neq i]}}), ..., (x_m, (-1)^{\\mathbb{1}_{[y_m \\neq i]}})$\\\n",
    "$\\text{ let } h_i = L(S_i)$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ }h_i(x)$\n",
    "\n",
    "\n",
    "Pseudocode for all-pairs (from textbook):\n",
    "\n",
    "Given inputs:\\\n",
    "training set $S = (x_1, y_1), ..., (x_m, y_m)$\\\n",
    "binary classifier - logistic regression $L$\n",
    "\n",
    "$\\text{foreach } i,j \\in Y \\text{ such that } i < j$\\\n",
    "$\\text{ initialize empty } S_{i,j}$\\\n",
    "$\\text{ for } t = 1, ..., m$\\\n",
    "$\\text{ }\\text{ If } y_t = i \\text{, then add } (x_t, 1) \\text{ to } S_{i,j}$\n",
    "$\\text{ }\\text{ If } y_t = j \\text{, then add } (x_t, -1) \\text{ to } S_{i,j}$\n",
    "$\\text{ let } h_{i,j} = L(S_{i,j}$\n",
    "\n",
    "Predicts:\\\n",
    "$ h(x) \\in argmax_{i \\in Y }\\text{ } (\\Sigma_{j \\in Y} \\text{ sign}(j-i) h_{i,j}(x))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa54a1c-c90e-488b-bdb1-265b147f26f8",
   "metadata": {},
   "source": [
    "### Loss: Logistic Loss + Regularization\n",
    "\n",
    "The loss function of a Logistic Regression classifier over $k$ classes is the **log-loss**, also called **cross-entropy loss**. Since we will only use binary classifier, e.g. Binary Logistic Regression, in this project, only **Binary Log Loss** will be introduced in this section.\n",
    "\n",
    "The Binary Log Loss on a sample of m data points, also called the Binary Cross Entropy Loss, is:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "The corresponding gradient of the Binary Log loss with respect to the model's wights is:\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "We also implement the L2 norm of wights to adpot Tikhonov regularization into our loss function. The L2 norm of wights is:\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "And the gradient of the L2 term with respect to the model's weights is:\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "\n",
    "In conclusion, the total loss function would be:\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))+ \\lambda\\sum_{i=1}^{d}w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622a64e-b029-4a79-8568-f9518a31350d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**One-vs-All** and **All-Pairs** are both strategies used to solve muticlass classification problems by utilizing binary classifiers. In this case, Stochastic Gradient Descent (Mini Batch) is a suitable choice.   \n",
    "In gradient descent, the general formula for weight update is:\n",
    "$$w_j = w_j - \\alpha \\cdot \\frac{\\partial L}{\\partial w_j}$$  \n",
    "\n",
    "For each batch of size $m$, the gradient of the binary log loss with respect to the weight is:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij}$$  \n",
    "\n",
    "If incorporate regularization (mentioned in the previous section), the total gradient becomes:\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i) - y_i) \\cdot x_{ij} + 2 \\lambda w_j$$\n",
    "Thus, the final weight update equation is: \n",
    "$$w_j = w_j - \\alpha \\cdot \\left( \\frac{\\partial L}{\\partial w_j} + 2 \\lambda w_j \\right)$$  \n",
    "\n",
    "Due to the nature of One-vs-All and All-pairs strategies, we apply this optimizer differently compared to direct multiclass classification techniques, such as multiclass logistic regression.  \n",
    "**One-vs-All**: for each class $j$, you train a seperate binary classifier that distinguishes class $j$ from all other classes.  \n",
    "**All-pairs**: for each unique class pair $(i, j)$, you train a binary classifier that differentiates between those two classes.  \n",
    "\n",
    "#### Pseudocode: Stochastic Gradient Descent for Logistic Regression (Lecture 6 Slide 21)  \n",
    "**Inputs**: Traning examples $S$, step size $\\alpha$, batch size $b < |S|$  \n",
    "Set converged false  \n",
    "**while** not converged:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Randomly shuffle $S$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $i = 0$ to $|S|/b - 1$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S'$ = Extracted current batch using $i$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\nabla L_{S'}(h_w)$ + regularization  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;converged = check_convergence$(S,w)$  \n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7202f8-caea-4a1c-b13f-cd9e159d7114",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803238f-5b7c-4b7f-a506-e471024bd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
    "\n",
    "def get_estimator(train_epochs, lr):\n",
    "    estimator = SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        tol=None,\n",
    "        max_iter=train_epochs,\n",
    "        shuffle=True,\n",
    "        random_state=0,\n",
    "        learning_rate='constant',\n",
    "        eta0=lr,\n",
    "        alpha=0)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24586b5",
   "metadata": {},
   "source": [
    "## Model: Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StandardScaler:\n",
    "    def _init_(self, X):\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1] # can include bias or not\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "    \n",
    "    def center(self, X):\n",
    "        return X - self.mean\n",
    "    \n",
    "    def scale(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        X_scaled = X_centered/self.std\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94ae8-cdc1-489f-a65f-d210e30a73ae",
   "metadata": {},
   "source": [
    "## **Model: Representation - Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add411a0-72b0-4f40-965f-2836c707cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    '''\n",
    "    Binary Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, batch_size=1, num_epochs=1, lr=0.0001, tol=1e-4):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.tol = tol\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        num_samples, num_features = X.shape\n",
    "        previous_loss = float('inf')\n",
    "        self.weights = np.zeros((1, num_features))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            shuffled_inds = np.random.permutation(num_samples)\n",
    "            shuffled_X = X[shuffled_inds]\n",
    "            shuffled_Y = Y[shuffled_inds]\n",
    "\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch = shuffled_X [start: min(end, num_samples)] \n",
    "                Y_batch = shuffled_Y [start: min(end, num_samples)] \n",
    "\n",
    "                predictions = sigmoid(np.dot(X_batch, self.weights.T)) # num_samples * 1 (num_classes)\n",
    "                Y_batch = np.reshape(Y_batch,(len(Y_batch),1)) # num_samples * 1, reshape Y to same dimensions of sigmoid\n",
    "                error = predictions - Y_batch\n",
    "                loss_grad = np.dot(error.T, X_batch)/len(X_batch)\n",
    "    \n",
    "                self.weights -= self.learning_rate * loss_grad\n",
    "            \n",
    "            current_loss = self.loss(X, Y)\n",
    "\n",
    "            # if abs(previous_loss - current_loss) < self.tol:\n",
    "            #     # print(f'Convergence reached at epoch {epoch + 1}')\n",
    "            #     break\n",
    "            \n",
    "\n",
    "            #print(f\"Epoch {epoch + 1}, Weights: {self.weights}\")\n",
    "                          \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Computes the logistic loss (binary cross-entropy loss) for binary classification\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        left_half = Y.T @ np.log(y_pred)\n",
    "        right_half = (1-Y).T @ np.log(1-y_pred)\n",
    "        \n",
    "        # Calculate the logistic loss\n",
    "        loss = -np.mean(left_half + right_half)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # X.shape: (batch size, num features)\n",
    "        # self.weights.shape: (1, num features)\n",
    "        dot_product = np.dot(self.weights, X.T) # n_classes * n_samples\n",
    "        probs = sigmoid(dot_product)\n",
    "        probsall = np.vstack((1-probs, probs)) # probs are for class 2\n",
    "        y_predict = np.argmax(probsall, axis=0) #finding the index of the max value in a column\n",
    "        return y_predict\n",
    "\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predicted_classes = self.predict(X)\n",
    "        return np.mean(predicted_classes == Y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Compute probabilities for the input data X.\n",
    "        @params:\n",
    "        X: A 2D Numpy array where each row contains an example\n",
    "        @return:\n",
    "        Probabilities of each example being in class 1\n",
    "        '''\n",
    "        dot_product = np.dot(self.weights, X.T) # n_classes * n_samples\n",
    "        probs = sigmoid(dot_product)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba488198",
   "metadata": {},
   "source": [
    "## **Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnevsAll:\n",
    "    def __init__(self, n_classes, batch_size=1, epochs=1, lr=0.01):\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        # self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # Split data and train each representation\n",
    "        self.S_Y = np.array([np.array(Y) for _ in range(self.n_classes)])\n",
    "        self.h = np.array([\n",
    "            MyLogisticRegression(\n",
    "                batch_size=self.batch_size, \n",
    "                num_epochs=self.epochs, \n",
    "                lr=self.lr, \n",
    "                tol=0) for _ in range(self.n_classes)])\n",
    "        \n",
    "        self.conv_epochs = [0] * self.n_classes\n",
    "        for cls in range(self.n_classes):\n",
    "            # Create S_i for each class i\n",
    "            S_Y_i = self.S_Y[cls]\n",
    "            cls_idx = S_Y_i == cls\n",
    "            S_Y_i[cls_idx] = 1\n",
    "            non_cls_idx = np.logical_not(cls_idx)\n",
    "            S_Y_i[non_cls_idx] = 0\n",
    "            \n",
    "            # Train h_i for each class i on S_i\n",
    "            h_i = self.h[cls]\n",
    "            conv_epoch = h_i.train(X, S_Y_i)\n",
    "            self.conv_epochs.append(conv_epoch)\n",
    "        \n",
    "            \n",
    "    # def loss(self, X, Y):\n",
    "    #     preds = self.predict(X)\n",
    "    #     # L1-loss\n",
    "    #     losses = np.abs(Y-preds)\n",
    "    #     losses = np.sum(losses)\n",
    "    #     return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # h_i in argmax h_i(x)\n",
    "        predictions = [0] * X.shape[0]\n",
    "        for i, x in enumerate(X):\n",
    "            preds = [0] * self.n_classes\n",
    "            # Get predictions from all hypotheses\n",
    "            for c in range(self.n_classes):\n",
    "                preds[c] = self.h[c].predict_proba(x)\n",
    "            # Select max prediction\n",
    "            predictions[i] = np.argmax(preds)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, preds, Y):\n",
    "        correct_preds = Y == preds\n",
    "        return np.sum(correct_preds) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09481c0",
   "metadata": {},
   "source": [
    "## **Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can only use python and numpy in this section.\n",
    "import numpy as np\n",
    "\n",
    "class AllPairs:\n",
    "    def __init__(self, n_classes, conv_threshold, batch_size, epochs, lr):\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.models = {}\n",
    "        self.lr = lr\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(i + 1, self.n_classes):\n",
    "                selected_indices = []\n",
    "                for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "                X_selected = X[selected_indices]\n",
    "                Y_selected = Y[selected_indices]\n",
    "                \n",
    "                for idx in range(len(Y_selected)):\n",
    "                    if Y_selected[idx] == i:\n",
    "                        Y_selected[idx] = 0\n",
    "                    else:\n",
    "                        Y_selected[idx] = 1\n",
    "\n",
    "                model = MyLogisticRegression(batch_size=self.batch_size, num_epochs=self.epochs, lr=self.lr, tol=self.conv_threshold)\n",
    "                model.train(X_selected, Y_selected)\n",
    "                key = i, j\n",
    "                self.models[key] = model\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Average Logistic loss?\n",
    "        total_loss = 0\n",
    "        for (i, j), model in self.models.items():\n",
    "            selected_indices = []\n",
    "            for index, label in enumerate(Y):\n",
    "                    if label == i or label == j:\n",
    "                        selected_indices.append(index)\n",
    "\n",
    "            X_selected = X[selected_indices]\n",
    "            Y_selected = Y[selected_indices]\n",
    "            \n",
    "            Y_selected = np.where(Y_selected == i, 0, 1)\n",
    "            total_loss += model.loss(X_selected, Y_selected)\n",
    "\n",
    "        return total_loss / len(self.models)\n",
    "        \n",
    "        '''\n",
    "        prediction = self.predict(X)\n",
    "        losses = np.abs(Y - prediction)\n",
    "        return np.sum(losses)\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], self.n_classes))\n",
    "        for key in self.models:\n",
    "            i, j = key\n",
    "            model = self.models[key]\n",
    "            prediction = model.predict(X)\n",
    "    \n",
    "            for idx in range(len(prediction)):\n",
    "                if prediction[idx] == 0:\n",
    "                    votes[idx, i] = votes[idx, i] + 1 \n",
    "                elif prediction[idx] == 1:\n",
    "                    votes[idx, j] = votes[idx, j] + 1\n",
    "        return np.argmax(votes, axis=1)\n",
    "        \n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd5d7-b3ba-4b43-8de9-b00cfcc2ee60",
   "metadata": {},
   "source": [
    "## **Check Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def CheckPlot_LR(X, Y, X_test, Y_test, lr, num_epochs):\n",
    "    '''\n",
    "    This is used for visualizing the differences of our logistic regression model\n",
    "    and the SGD classifier from sklearn.\n",
    "    '''\n",
    "    # train my models\n",
    "    X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "    my_model = MyLogisticRegression(lr=lr, batch_size=1, num_epochs=num_epochs)\n",
    "    my_model.train(X_bias, Y)\n",
    "    \n",
    "    # train sklearn model\n",
    "    # SGDClassifier is always batch_size == 1\n",
    "    sklearn_model = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    tol=None,\n",
    "    max_iter=num_epochs,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    learning_rate='constant',\n",
    "    eta0=lr,\n",
    "    alpha=0)\n",
    "    sklearn_model.fit(X, Y)\n",
    "\n",
    "    weights = my_model.weights\n",
    "    assert isinstance(weights, np.ndarray)\n",
    "    assert weights.ndim==2 and weights.shape == (1,X.shape[1]+1)\n",
    "    # FIXME: relative tolerance might be not strict enough\n",
    "    print('my weight', weights)\n",
    "    print('sklearn weight', sklearn_model.coef_[0], sklearn_model.intercept_[0])\n",
    "    assert weights[0][:-1] == pytest.approx(sklearn_model.coef_[0], 0.01)\n",
    "    assert weights[0][-1] == pytest.approx(sklearn_model.intercept_[0], 0.01)\n",
    "    \n",
    "    # Create a meshgrid and predict on the grid points\n",
    "    x_min = min(X[:, 0].min(), X_test[:, 0].min())\n",
    "    x_max = max(X[:, 0].max(), X_test[:, 0].max())\n",
    "    y_min = min(X[:, 1].min(), X_test[:, 1].min())\n",
    "    y_max = max(X[:, 1].max(), X_test[:, 1].max())\n",
    "    x_gap = x_max - x_min\n",
    "    y_gap = y_max - y_min\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min -x_gap/100, x_max + x_gap/100, 200), np.linspace(y_min-y_gap/100, y_max + y_gap/100, 200))\n",
    "    bias = np.ones(xx.ravel().shape) \n",
    "    features_with_bias = np.c_[xx.ravel(), yy.ravel(), bias] \n",
    "\n",
    "    my_Z = my_model.predict(features_with_bias)\n",
    "    my_Z = my_Z.reshape(xx.shape)\n",
    "\n",
    "    sklearn_Z = sklearn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    sklearn_Z = sklearn_Z.reshape(xx.shape)\n",
    "\n",
    "    # check test data predictions\n",
    "    X_test_bias = np.c_[X_test, np.ones(X_test.shape[0])]\n",
    "    my_preds = my_model.predict(X_test_bias)\n",
    "    sklearn_preds = sklearn_model.predict(X_test)\n",
    "    assert (my_preds == sklearn_preds).all()\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot my model's results\n",
    "    axes[0].contourf(xx, yy, my_Z, alpha=0.3, colors=['blue', 'red'], levels=[0, 0.5, 1])\n",
    "    train_data_label1 = X[np.where(Y == 1)]\n",
    "    train_data_label0 = X[np.where(Y == 0)]\n",
    "    test_data_label1 = X_test[np.where(Y_test == 1)]\n",
    "    test_data_label0 = X_test[np.where(Y_test == 0)]\n",
    "    axes[0].scatter(train_data_label1[:, 0], train_data_label1[:, 1], c='red', edgecolor='k', label='Train Data Label 1')\n",
    "    axes[0].scatter(train_data_label0[:, 0], train_data_label0[:, 1], c='blue', edgecolor='k', label='Train Data Label 0')\n",
    "    axes[0].scatter(test_data_label1[:, 0], test_data_label1[:, 1], c='red', edgecolor='k', label='Test Data Label 1', marker='*')\n",
    "    axes[0].scatter(test_data_label0[:, 0], test_data_label0[:, 1], c='blue', edgecolor='k', label='Test Data Label 0', marker='*')\n",
    "    axes[0].set_title(\"Decision Boundary (My Model)\")\n",
    "    axes[0].set_xlabel(\"Feature 1\")\n",
    "    axes[0].set_ylabel(\"Feature 2\")\n",
    "    axes[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Plot sklearn model's results\n",
    "    axes[1].contourf(xx, yy, sklearn_Z, alpha=0.3, colors=['blue', 'red'], levels=[0, 0.5, 1])\n",
    "    axes[1].scatter(train_data_label1[:, 0], train_data_label1[:, 1], c='red', edgecolor='k', label='Train Data Label 1')\n",
    "    axes[1].scatter(train_data_label0[:, 0], train_data_label0[:, 1], c='blue', edgecolor='k', label='Train Data Label 0')\n",
    "    axes[1].scatter(test_data_label1[:, 0], test_data_label1[:, 1], c='red', edgecolor='k', label='Test Data Label 1', marker='*')\n",
    "    axes[1].scatter(test_data_label0[:, 0], test_data_label0[:, 1], c='blue', edgecolor='k', label='Test Data Label 0', marker='*')\n",
    "    axes[1].set_title(\"Decision Boundary (Sklearn Model)\")\n",
    "    axes[1].set_xlabel(\"Feature 1\")\n",
    "    axes[1].set_ylabel(\"Feature 2\")\n",
    "    axes[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    my_acc = my_model.accuracy(X=X_test_bias, Y=Y_test)\n",
    "    sklearn_acc = sklearn_model.score(X=X_test,y=Y_test)\n",
    "    print('The Accuracy of My Logistic Regression Model is ', my_acc)\n",
    "    print('The Accuracy of Sklearn SGD Classifier is ', sklearn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f9ada",
   "metadata": {},
   "source": [
    "### 5-Point Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044782d7-d5ee-45ec-bbbb-fdb593d53a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create Data\n",
    "X1_train = np.array([[0,4], [0,3], [5,0], [4,1], [0,5]])\n",
    "Y1_train = np.array([0,0,1,1,0])\n",
    "\n",
    "X1_test = np.array([[0,0], [-5,3], [9,0], [1,0], [6,-7]])\n",
    "Y1_test = np.array([0,0,1,0,1])\n",
    "\n",
    "CheckPlot_LR(X=X1_train, Y=Y1_train, X_test=X1_test, Y_test=Y1_test, lr=0.01, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e8702",
   "metadata": {},
   "source": [
    "### Linearly separated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# generate two linear seperated datasets\n",
    "class_0 = np.random.rand(100, 2) + [0, 1]  \n",
    "class_1 = np.random.rand(100, 2) + [1, 0]  \n",
    "\n",
    "# add noises\n",
    "noise_0 = np.random.normal(0, 0.1, class_0.shape)\n",
    "noise_1 = np.random.normal(0, 0.1, class_1.shape)\n",
    "class_0 += noise_0\n",
    "class_1 += noise_1\n",
    "\n",
    "X2 = np.vstack((class_0, class_1))\n",
    "Y2 = np.hstack((np.zeros(100), np.ones(100)))\n",
    "\n",
    "# Split train and test datasets\n",
    "indices = np.arange(X2.shape[0])\n",
    "shuffled_inds = np.random.permutation(indices)\n",
    "X2 = X2[shuffled_inds]\n",
    "Y2 = Y2[shuffled_inds]\n",
    "X2_train = X2[indices[:150]]\n",
    "Y2_train = Y2[indices[:150]]\n",
    "X2_test = X2[indices[-51:-1]]\n",
    "Y2_test = Y2[indices[-51:-1]]\n",
    "\n",
    "# check\n",
    "CheckPlot_LR(X=X2_train, Y=Y2_train, X_test=X2_test, Y_test=Y2_test, \n",
    "             lr=0.01, num_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f230b75",
   "metadata": {},
   "source": [
    "### Non-Linearly separated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate circle-like datasets\n",
    "X3, Y3 = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=0)\n",
    "indices = np.arange(X3.shape[0])\n",
    "shuffled_inds = np.random.permutation(indices)\n",
    "X3 = X3[shuffled_inds]\n",
    "Y3 = Y3[shuffled_inds]\n",
    "X3_train = X3[indices[:150]]\n",
    "Y3_train = Y3[indices[:150]]\n",
    "X3_test = X3[indices[-51:-1]]\n",
    "Y3_test = Y3[indices[-51:-1]]\n",
    "\n",
    "# check\n",
    "CheckPlot_LR(X=X3_train, Y=Y3_train, X_test=X3_test, Y_test=Y3_test, \n",
    "             lr=0.01, num_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572c47f",
   "metadata": {},
   "source": [
    "### High Dimensional Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a high-dimensional dataset for binary classification\n",
    "X4, Y4 = make_classification(\n",
    "    n_samples=300,          # Number of samples\n",
    "    n_features=10,          # Total number of features\n",
    "    n_informative=8,        # Number of informative features\n",
    "    n_redundant=1,          # Number of redundant features\n",
    "    n_classes=2,             # Binary classification\n",
    "    random_state=0          # Reproducibility\n",
    ")\n",
    "\n",
    "indices = np.arange(X4.shape[0])\n",
    "shuffled_inds = np.random.permutation(indices)\n",
    "X4 = X4[shuffled_inds]\n",
    "Y4 = Y4[shuffled_inds]\n",
    "X4_train = X4[indices[:225]]\n",
    "Y4_train = Y4[indices[:225]]\n",
    "X4_test = X4[indices[-76:-1]]\n",
    "Y4_test = Y4[indices[-76:-1]]\n",
    "\n",
    "# set parameters for the learning\n",
    "num_epochs = 1000\n",
    "lr =0.01\n",
    "X4_train_bias = np.c_[X4_train, np.ones(X4_train.shape[0])]\n",
    "X4_test_bias = np.c_[X4_test, np.ones(X4_test.shape[0])]\n",
    "my_model = MyLogisticRegression(lr=lr, batch_size=1, num_epochs=num_epochs)\n",
    "my_model.train(X4_train_bias, Y4_train)\n",
    "\n",
    "# train sklearn model\n",
    "# SGDClassifier is always batch_size == 1\n",
    "sklearn_model = SGDClassifier(\n",
    "loss='log_loss',\n",
    "tol=None,\n",
    "max_iter=num_epochs,\n",
    "shuffle=True,\n",
    "random_state=0,\n",
    "learning_rate='constant',\n",
    "eta0=lr,\n",
    "alpha=0)\n",
    "sklearn_model.fit(X4_train, Y4_train)\n",
    "\n",
    "weights = my_model.weights\n",
    "assert isinstance(weights, np.ndarray)\n",
    "assert weights.ndim==2 and weights.shape == (1,X4_train.shape[1]+1)\n",
    "\n",
    "# print(weights)\n",
    "# print(sklearn_model.coef_[0])\n",
    "# print(sklearn_model.intercept_[0])\n",
    "assert weights[0][:-1] == pytest.approx(sklearn_model.coef_[0], 0.01)\n",
    "assert weights[0][-1] == pytest.approx(sklearn_model.intercept_[0], 0.01)\n",
    "\n",
    "my_preds = my_model.predict(X4_test_bias)\n",
    "sklearn_preds = sklearn_model.predict(X4_test)\n",
    "assert (my_preds == sklearn_preds).all()\n",
    "\n",
    "'''\n",
    "We can't reproduce the results when the dimension of the feature space is high\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc0487-1397-47de-a8e8-5b5a9bb18e08",
   "metadata": {},
   "source": [
    "## **Check Model: one-vs-all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b137b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def CheckPlot_OnevsAll(X_train, Y_train, X_test, Y_test, lr, num_epochs):\n",
    "    X_train_bias = np.c_[X_train, np.ones(X_train.shape[0])]\n",
    "    X_test_bias = np.c_[X_test, np.ones(X_test.shape[0])]\n",
    "\n",
    "    # initialize model\n",
    "    n_classes = len(np.unique(Y_train))\n",
    "    my_model = OnevsAll(n_classes, epochs=num_epochs, lr=lr)\n",
    "    estimator = get_estimator(num_epochs, lr)\n",
    "    sklearn_model = OneVsRestClassifier(estimator)\n",
    "\n",
    "    my_model.train(X_train_bias, Y_train)\n",
    "    sklearn_model.fit(X_train, Y_train)\n",
    "\n",
    "    # generate meshgrids and predict on it\n",
    "    x_min = min(X_train[:, 0].min(), X_test[:, 0].min())\n",
    "    x_max = max(X_train[:, 0].max(), X_test[:, 0].max())\n",
    "    y_min = min(X_train[:, 1].min(), X_test[:, 1].min())\n",
    "    y_max = max(X_train[:, 1].max(), X_test[:, 1].max())\n",
    "    x_gap = x_max - x_min\n",
    "    y_gap = y_max - y_min\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min -x_gap/100, x_max + x_gap/100, 200), np.linspace(y_min-y_gap/100, y_max + y_gap/100, 200))\n",
    "    bias = np.ones(xx.ravel().shape) \n",
    "    features_with_bias = np.c_[xx.ravel(), yy.ravel(), bias] \n",
    "\n",
    "    my_Z = np.array(my_model.predict(features_with_bias))\n",
    "    my_Z = my_Z.reshape(xx.shape)\n",
    "\n",
    "    sklearn_Z = sklearn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    sklearn_Z = sklearn_Z.reshape(xx.shape)\n",
    "\n",
    "    # check test data predictions\n",
    "    my_preds = my_model.predict(X_test_bias)\n",
    "    sklearn_preds = sklearn_model.predict(X_test)\n",
    "    assert (my_preds == sklearn_preds).all()\n",
    "\n",
    "    # Step 6: Visualize the decision boundaries\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Custom OvA model\n",
    "    axes[0].contourf(xx, yy, my_Z.reshape(xx.shape), alpha=0.3, cmap=\"coolwarm\")\n",
    "    axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                    c=Y_train, edgecolor=\"k\",  cmap=\"coolwarm\", label='Train Data')\n",
    "    axes[0].scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c=Y_test, edgecolor=\"k\",  cmap=\"coolwarm\", marker='*', label='Test Data')    \n",
    "    axes[0].set_title(\"My One-vs-All Model\")\n",
    "    axes[0].set_xlabel(\"Feature 1\")\n",
    "    axes[0].set_ylabel(\"Feature 2\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Sklearn OvA model\n",
    "    axes[1].contourf(xx, yy, sklearn_Z.reshape(xx.shape), alpha=0.3, cmap=\"coolwarm\")\n",
    "    axes[1].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                    c=Y_train, edgecolor=\"k\",  cmap=\"coolwarm\", label='Train Data')\n",
    "    axes[1].scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c=Y_test, edgecolor=\"k\",  cmap=\"coolwarm\", marker='*', label='Test Data')    \n",
    "    axes[1].set_title(\"Sklearn One-vs-Rest Model\")\n",
    "    axes[1].set_xlabel(\"Feature 1\")\n",
    "    axes[1].set_ylabel(\"Feature 2\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Point Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523621b-f962-4448-8281-04e629807402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.array([[0,4], [0,3], [5,0], [4,1], [0,5], [1,0], [2,1], [3,2], [4,3], [5,4]])\n",
    "X_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1], [1,0,1], [2,1,1], [3,2,1], [4,3,1], [5,4,1]])\n",
    "Y = np.array([0,0,0,1,1,1,1,2,2,2])\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "# Initialize models:\n",
    "train_epochs = 10000\n",
    "lr = 0.01\n",
    "my_model = OnevsAll(n_classes, epochs=train_epochs, lr=lr)\n",
    "\n",
    "estimator = get_estimator(train_epochs, lr)\n",
    "sklearn_model = OneVsRestClassifier(estimator)\n",
    "\n",
    "my_model.train(X_bias, Y)\n",
    "sklearn_model.fit(X, Y)\n",
    "\n",
    "# Check that S is populated correctly\n",
    "assert my_model.S_Y.shape[0] == n_classes\n",
    "assert my_model.S_Y.shape[1] == Y.shape[0]\n",
    "\n",
    "# Check h (individual classifiers)\n",
    "assert len(my_model.h) == n_classes\n",
    "assert len(sklearn_model.estimators_) == n_classes\n",
    "for h in my_model.h:\n",
    "    assert isinstance(h, MyLogisticRegression)\n",
    "for i in range(n_classes):\n",
    "    my_weights = my_model.h[i].weights[0][:-1]\n",
    "    my_bias = my_model.h[i].weights[0][-1]\n",
    "    sklearn_weights = sklearn_model.estimators_[i].coef_[0]\n",
    "    sklearn_bias = sklearn_model.estimators_[i].intercept_[0]\n",
    "    print(\" === \", i)\n",
    "    print(my_weights)\n",
    "    print(sklearn_weights)\n",
    "    print(my_bias)\n",
    "    print(sklearn_bias)\n",
    "\n",
    "    # assert my_weights == pytest.approx(sklearn_weights, 0.01)\n",
    "    # assert my_bias == pytest.approx(sklearn_bias, 0.01)\n",
    "\n",
    "# Check predictions\n",
    "predictions = my_model.predict(X_bias)\n",
    "print(\"My Predictions:\", np.array(predictions))\n",
    "\n",
    "sklearn_predictions = sklearn_model.predict(X)\n",
    "print(\"sklearn Predictions:\", sklearn_predictions)\n",
    "\n",
    "print('num_samples', X.shape[0])\n",
    "print('Differences', np.sum(np.abs(np.array(predictions)-sklearn_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9ea85",
   "metadata": {},
   "source": [
    "### Linearly-separated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate 100 samples for each class\n",
    "n_samples = 100\n",
    "\n",
    "# class 0 centered on [1,1]\n",
    "class_1 = np.random.randn(n_samples, 2) + [1, 1]\n",
    "\n",
    "# class 1 centered on [6,6]\n",
    "class_2 = np.random.randn(n_samples, 2) + [5, 5]\n",
    "\n",
    "# class 2 centered on [3,4]\n",
    "class_3 = np.random.randn(n_samples, 2) + [1, 9]\n",
    "\n",
    "\n",
    "X5 = np.vstack([class_1, class_2, class_3])\n",
    "Y5 = np.hstack([np.zeros(n_samples), np.ones(n_samples), np.full(n_samples, 2)]) \n",
    "\n",
    "indices = np.arange(X5.shape[0])\n",
    "shuffled_inds = np.random.permutation(indices)\n",
    "X5 = X5[shuffled_inds]\n",
    "Y5 = Y5[shuffled_inds]\n",
    "X5_train = X5[indices[:225]]\n",
    "Y5_train = Y5[indices[:225]]\n",
    "X5_test = X5[indices[-76:-1]]\n",
    "Y5_test = Y5[indices[-76:-1]]\n",
    "\n",
    "#check\n",
    "CheckPlot_OnevsAll(X5_train, Y5_train, X5_test, Y5_test, lr=0.01, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2f0ae",
   "metadata": {},
   "source": [
    "### Non-linearly Separated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons\n",
    "np.random.seed(0)\n",
    "\n",
    "x1, y1 = make_circles(n_samples=150, noise=0.05, factor=0.5, random_state=0)\n",
    "x2, y2 = make_moons(n_samples=150, noise=0.05, random_state=0)\n",
    "x2 += np.array([2.5, 0]) \n",
    "y2 += 2 \n",
    "\n",
    "X6 = np.vstack([x1, x2])\n",
    "Y6 = np.hstack([y1, y2])\n",
    "\n",
    "indices = np.arange(X5.shape[0])\n",
    "shuffled_inds = np.random.permutation(indices)\n",
    "X6 = X6[shuffled_inds]\n",
    "Y6 = Y6[shuffled_inds]\n",
    "X6_train = X6[indices[:225]]\n",
    "Y6_train = Y6[indices[:225]]\n",
    "X6_test = X6[indices[-76:-1]]\n",
    "Y6_test = Y6[indices[-76:-1]]\n",
    "\n",
    "#check\n",
    "CheckPlot_OnevsAll(X6_train, Y6_train, X6_test, Y6_test, lr=0.01, num_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe72db-d6e7-44e2-ade1-991a5658a1b5",
   "metadata": {},
   "source": [
    "## **Check Model: all-pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e774e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def CheckPlot_AllPairs(X_train, Y_train, X_test, Y_test, lr, num_epochs):\n",
    "    X_train_bias = np.c_[X_train, np.ones(X_train.shape[0])]\n",
    "    X_test_bias = np.c_[X_test, np.ones(X_test.shape[0])]\n",
    "\n",
    "    # initialize model\n",
    "    n_classes = len(np.unique(Y_train))\n",
    "    my_model = AllPairs(n_classes=n_classes, conv_threshold=0.001, \n",
    "                        batch_size=1, epochs=num_epochs, lr=lr)\n",
    "    sklearn_model = OneVsOneClassifier(LinearSVC(random_state=0))\n",
    "\n",
    "    my_model.train(X_train_bias, Y_train)\n",
    "    sklearn_model.fit(X_train, Y_train)\n",
    "\n",
    "    # generate meshgrids and predict on it\n",
    "    x_min = min(X_train[:, 0].min(), X_test[:, 0].min())\n",
    "    x_max = max(X_train[:, 0].max(), X_test[:, 0].max())\n",
    "    y_min = min(X_train[:, 1].min(), X_test[:, 1].min())\n",
    "    y_max = max(X_train[:, 1].max(), X_test[:, 1].max())\n",
    "    x_gap = x_max - x_min\n",
    "    y_gap = y_max - y_min\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min -x_gap/100, x_max + x_gap/100, 200), np.linspace(y_min-y_gap/100, y_max + y_gap/100, 200))\n",
    "    bias = np.ones(xx.ravel().shape) \n",
    "    features_with_bias = np.c_[xx.ravel(), yy.ravel(), bias] \n",
    "\n",
    "    my_Z = np.array(my_model.predict(features_with_bias))\n",
    "    my_Z = my_Z.reshape(xx.shape)\n",
    "\n",
    "    sklearn_Z = sklearn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    sklearn_Z = sklearn_Z.reshape(xx.shape)\n",
    "\n",
    "    # check test data predictions\n",
    "    my_preds = my_model.predict(X_test_bias)\n",
    "    sklearn_preds = sklearn_model.predict(X_test)\n",
    "    assert (my_preds == sklearn_preds).all()\n",
    "\n",
    "    # Step 6: Visualize the decision boundaries\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Custom OvA model\n",
    "    axes[0].contourf(xx, yy, my_Z.reshape(xx.shape), alpha=0.3, cmap=\"coolwarm\")\n",
    "    axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                    c=Y_train, edgecolor=\"k\",  cmap=\"coolwarm\", label='Train Data')\n",
    "    axes[0].scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c=Y_test, edgecolor=\"k\",  cmap=\"coolwarm\", marker='*', label='Test Data')    \n",
    "    axes[0].set_title(\"My AllPair Model\")\n",
    "    axes[0].set_xlabel(\"Feature 1\")\n",
    "    axes[0].set_ylabel(\"Feature 2\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Sklearn OvA model\n",
    "    axes[1].contourf(xx, yy, sklearn_Z.reshape(xx.shape), alpha=0.3, cmap=\"coolwarm\")\n",
    "    axes[1].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                    c=Y_train, edgecolor=\"k\",  cmap=\"coolwarm\", label='Train Data')\n",
    "    axes[1].scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c=Y_test, edgecolor=\"k\",  cmap=\"coolwarm\", marker='*', label='Test Data')    \n",
    "    axes[1].set_title(\"Sklearn Allpair Model\")\n",
    "    axes[1].set_xlabel(\"Feature 1\")\n",
    "    axes[1].set_ylabel(\"Feature 2\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d9095",
   "metadata": {},
   "source": [
    "### Linearly Seperated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPlot_AllPairs(X5_train, Y5_train, X5_test, Y5_test, 0.01, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4650f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPlot_AllPairs(X6_train, Y6_train, X6_test, Y6_test, 0.001, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13ec59",
   "metadata": {},
   "source": [
    "## **Performance on Iris Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a101fef",
   "metadata": {},
   "source": [
    "### One-vs-all Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "bias_col = np.ones((X.shape[0], 1))\n",
    "X_biased = np.hstack((X, bias_col))\n",
    "\n",
    "Y = data.target\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "# Initialize models:\n",
    "train_epochs = 5000\n",
    "lr = 0.01\n",
    "my_model = OnevsAll(n_classes, epochs=train_epochs, lr=lr)\n",
    "my_model.train(X_biased, Y)\n",
    "\n",
    "estimator = get_estimator(train_epochs, lr)\n",
    "sklearn_model = OneVsRestClassifier(estimator)\n",
    "sklearn_model.fit(X, Y)\n",
    "\n",
    "my_preds = my_model.predict(X_biased)\n",
    "sklearn_preds = sklearn_model.predict(X)\n",
    "\n",
    "print(\"PredictionsA:\", np.array(my_preds))\n",
    "print(\"PredictionsB:\", sklearn_preds)\n",
    "\n",
    "print('num_samples', X.shape[0])\n",
    "print('Differences', np.sum(np.abs(np.array(my_preds)-sklearn_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e33d4",
   "metadata": {},
   "source": [
    "### All-Pairs Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18136c5-fe5d-4fc6-9436-7988a32a13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "bias_col = np.ones((X.shape[0], 1))\n",
    "X_biased = np.hstack((X, bias_col))\n",
    "\n",
    "Y = data.target\n",
    "n_classes = len(np.unique(Y))\n",
    "\n",
    "model = AllPairs(n_classes=n_classes, conv_threshold=0.001, batch_size=10, epochs=5000, lr=0.01)\n",
    "model.train(X_biased, Y)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(X_biased)\n",
    "\n",
    "\n",
    "print(\"PredictionsA:\", np.array(predictions))\n",
    "\n",
    "sklearn_predictions = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y).predict(X)\n",
    "print(\"PredictionsB:\", sklearn_predictions)\n",
    "\n",
    "print('num_samples', X.shape[0])\n",
    "print('Differences', np.sum(np.abs(np.array(predictions)-sklearn_predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DATA2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
